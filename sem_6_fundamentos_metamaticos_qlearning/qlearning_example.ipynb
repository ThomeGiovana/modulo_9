{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DySQ6mv302p6"
      },
      "source": [
        "# Questão Ponderada: Fundamentos Matemáticos para Métodos de Diferença Temporal (TD): Q-Learning e Sarsa (Parte 2)\n",
        "\n",
        "Giovana Lisbôa Thomé\n",
        "\n",
        "Março 2024\n",
        "\n",
        "Módulo 9 - Ciência da Computação - Inteli"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "p7EP5B3OqTyn"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "U2OZhTaGmeSa"
      },
      "outputs": [],
      "source": [
        "STATE_A = 0\n",
        "STATE_B = 1\n",
        "STATE_C = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "_39eWFAAmnXN"
      },
      "outputs": [],
      "source": [
        "GO_TO_B = 0\n",
        "GO_TO_C = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "DlvIljx5kp8L"
      },
      "outputs": [],
      "source": [
        "def calculateQValue(oldQ, reward, maxNextPossibleQValue):\n",
        "  discountRate = 0.9\n",
        "  learningRate = 0.1\n",
        "  newQ = oldQ + learningRate * (reward + (discountRate * maxNextPossibleQValue) - oldQ)\n",
        "  return newQ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "5fkJlsAJmy1F"
      },
      "outputs": [],
      "source": [
        "def calculateReward(action, state):\n",
        "  if state == STATE_A:\n",
        "    if action == GO_TO_B:\n",
        "      return 0.5, STATE_B\n",
        "    if action == GO_TO_C:\n",
        "      return 1, STATE_C\n",
        "  elif state == STATE_B:\n",
        "    return 2, STATE_C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "2Abv_nt5nmAE"
      },
      "outputs": [],
      "source": [
        "def alternateAction(action):\n",
        "  if action == GO_TO_B:\n",
        "    return GO_TO_C\n",
        "  elif action == GO_TO_C:\n",
        "    return GO_TO_B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoZyqE-BjkR2",
        "outputId": "0f3c5c9b-d47b-48e2-cece-616724f7e68c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[2.2994076808048067, 0.9999734386011123], [1.9999468772022246]]\n"
          ]
        }
      ],
      "source": [
        "from os import execle\n",
        "qValueTable = [\n",
        "    [0, 0], # qualidades das ações do estado A\n",
        "    [0] # qualidades das ações do estado B\n",
        "]\n",
        "\n",
        "i = 0\n",
        "currentState = STATE_A\n",
        "action = GO_TO_B\n",
        "\n",
        "while i < 200: # metade das iterações para cada caminho\n",
        "\n",
        "  # estado A\n",
        "  if action == GO_TO_B:\n",
        "    # ir para B\n",
        "    reward, nextState = calculateReward(action, currentState)\n",
        "    newQ = calculateQValue(qValueTable[0][0], reward, np.max(qValueTable[nextState]))\n",
        "    qValueTable[0][0] = newQ\n",
        "    currentState = nextState\n",
        "\n",
        "    # em seguinda, ir para C\n",
        "    reward, nextState = calculateReward(GO_TO_C, currentState)\n",
        "    newQ = calculateQValue(qValueTable[1][0], reward, 0)\n",
        "    qValueTable[1][0] = newQ\n",
        "    currentState = nextState\n",
        "\n",
        "  elif action == GO_TO_C:\n",
        "    # ir direto para C\n",
        "    reward, nextState = calculateReward(action, currentState)\n",
        "    newQ = calculateQValue(qValueTable[0][1], reward, 0)\n",
        "    qValueTable[0][1] = newQ\n",
        "    currentState = nextState\n",
        "\n",
        "  currentState = STATE_A\n",
        "  # alternando estre as duas escolhas de caminho possíveis\n",
        "  nextAction = alternateAction(action)\n",
        "  action = nextAction\n",
        "  i += 1\n",
        "\n",
        "print(qValueTable)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEmwRX9VxHAQ"
      },
      "source": [
        "O algoritmo escrito está hardcoded, ou seja, não se encaixará em outros contextos de ML. É apenas para fins de aprendizagem e para resolver a atividade ponderada Questão Ponderada: Fundamentos Matemáticos para Métodos de Diferença Temporal (TD): Q-Learning e Sarsa (Parte 2)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
